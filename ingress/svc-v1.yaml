## service controller creates target pool as backend if we use load balancer (external network pass through load balancer) and instance group as backend when use internal load balancer
# Target Pools: Used for legacy, external, passthrough load balancers (simple, direct Layer 4 TCP/UDP forwarding).
#Instance Groups: Used for internal load balancers (and all modern backend-service-based load balancers), supporting more features (e.g., health checks, autoscaling).
apiVersion: v1
kind: Service
metadata:
  name: svc-v1
  annotations:
      cloud.google.com/backend-config: '{"ports": {"http":"v1"}}'
    #networking.gke.io/load-balancer-type: "Internal"
    #networking.gke.io/internal-load-balancer-allow-global-access: "true" # so that instances from other region can able to access it but within same vpc only
    #cloud.google.com/neg: '{"exposed_ports": {"80":{}}}' - it enables cloud native load balancing. this port should be same as service port.load balaning directly sends traffic to pods. but there is drawback wit this approach
spec:
  #type: NodePort
  #type: ClusterIP
  type: ClusterIP
  selector:
    app: v1
  ports:
  - port: 80
    targetPort: http

# GCE external creates zonal NEGs → ✅ Correct

# GCE internal creates MIGs → ✅ Only if you're not using NEG annotations

# Ingress Type	Backend Used	Uses NEG?	Uses MIG?	Comment
# GCE External	Zonal NEGs (Pods)	✅ Yes	❌ No	Default behavior
# GCE Internal	MIGs (NodePorts)	❌ No	✅ Yes	Unless you use NEG annotation
# GCE Internal + NEG	Zonal NEGs (Pods)	✅ Yes	❌ No	Modern setup

  
